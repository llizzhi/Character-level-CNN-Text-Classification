{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\abc\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.python.framework import ops\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach Description\n",
    "The approach I used to solve this text classification challenge is based on the paper *Character-level Convolutional Networks for Text Classification*, published by Xiang ZHANG, Junbo ZHU, and Yann LECUN in 2016. In this project, text is treated as a kind of raw signal at character level, and one-dimensional ConvNets is applied on it. Therefore, this method does not require the knowledge about the syntactic or semantic structure of a language.<br>\n",
    "<br>\n",
    "Figure 1 describes the general process of the model. First of all, each line of text would be converted into a matrix with a size ($l$, $s$, 1) by using one-hot encoding, where $l$ represents the maximum length of a line of text in the training set, and $s$ represents the number of unique alphabets in the training set. Secondly, the matrix would be fed into a ConvNets model, and a output with a size of (1, 12) is expected to indicate the probability that this sample belonging to category 0-12 respectively. Finally, for each sample, the category with highest probability is selected as the predicted class of that sample.\n",
    "\n",
    "<center><img src=\"images/CNN.jpg\" style=\"width:650px;height:200px;\"></center>\n",
    "<center>Figure 1 Illustration of the model</center>\n",
    "<br>\n",
    "Reference: https://papers.nips.cc/paper/5782-character-level-convolutional-networks-for-text-classification.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1: Data Exploration and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xtrain = open('xtrain_obfuscated.txt', 'r').read().split('\\n')[:-1]\n",
    "xtest = open('xtest_obfuscated.txt', 'r').read().split('\\n')[:-1]\n",
    "ytrain = open('ytrain.txt', 'r').read().split('\\n')[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1.1 Check the number of unique alphabets in the training set **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 27 unique characters in your data.\n"
     ]
    }
   ],
   "source": [
    "data = open('xtrain_obfuscated.txt', 'r').read()\n",
    "chars = list(set(data))\n",
    "vocab_size = len(chars)\n",
    "print('There are %d unique characters in your data.' % (vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The characters are a-z (26 characters) plus the \"\\n\", which indicates the end of a line of text. In the cell below, a python dictionary is created to map each character to an index from 0-26, which would be used to represent the column index each alphabet belong to in the input matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n"
     ]
    }
   ],
   "source": [
    "char_to_ix = { ch:i for i,ch in enumerate(sorted(chars)) }\n",
    "print(char_to_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1.2 Check the maximum length of string in the training set **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum length of string in your dataset is 452\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = max([len(i) for i in xtrain])\n",
    "print('The maximum length of string in your dataset is %d' % (max_seq_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1.3 Check the distribution of each class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of value 0:543\n",
      "number of value 1:3459\n",
      "number of value 2:1471\n",
      "number of value 3:4023\n",
      "number of value 4:2337\n",
      "number of value 5:2283\n",
      "number of value 6:4226\n",
      "number of value 7:5097\n",
      "number of value 8:3634\n",
      "number of value 9:980\n",
      "number of value 10:3052\n",
      "number of value 11:1408\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,12):\n",
    "    print('number of value ' + str(i) + ':' + str(ytrain.count(str(i))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of each class in the training set is relatively imbalanced, while the largest class (class 7) has 5097 samples, accounting for 15.68% of the training set, and the smallest class(class 0) has 543 samples, accounting for merely 1.67% of the training set. According to past experiment, distribution of the training data has a big impact on ConvNets performance. A relationship was found between larger imbalances and worse performances when some classes were over-represented. <br>\n",
    "Reference: https://www.kth.se/social/files/588617ebf2765401cfcc478c/PHensmanDMasko_dkand15.pdf\n",
    "\n",
    "Using the sampling technique oversampling on the imbalanced data increased the CNN performances to that of the ConvNets trained with balanced data. Therefore, a sampling method is adopted in this projected to mitigate this imbalance problem. Figure 2 shows the general strategy of balancing the dataset.\n",
    "<center><img src=\"images/DataProcessing.png\" style=\"width:700px;height:400px;\"></center>\n",
    "<center>Figure 2 Illustration of sampling</center>\n",
    "<br>\n",
    "\n",
    "Here, I want to generate a data set that each of the 12 categories has 4500 training samples and 500 validation samples respectively, so the total training set will have 54000 samples and validation set will have 6000 samples. To achieve it, samples with same label $i$ will be split into original training set(90%) and testing set(10%). If the size of original training set is larger than 4500, or the size of validation set is larger than 500, I will sample 4500 line of text from the original training set as new training set, and 500 line of text from the original validation set as new validation set. In contrast, if the size of original training set for a category is less than 4500, or the size of the original validation set is less than 500, an oversampling method would be adopted to generate new samples belonging to same class. Figure 3 depicts the method used to generate a new sample based on a random existing sample.\n",
    "<center><img src=\"images/OverSampling.png\" style=\"width:650px;height:250px;\"></center>\n",
    "<center>Figure 3 Illustration of oversampling</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sampling(category, training_size, testing_size):\n",
    "    x_train_sub = []\n",
    "    y_train_sub = []\n",
    "    x_valid_sub = []\n",
    "    y_valid_sub = []\n",
    "\n",
    "    index = [pos for pos, value in enumerate(ytrain) if value == str(category)]\n",
    "    number = len(index)\n",
    "    train_size = round(number * 0.9)\n",
    "\n",
    "    train_idx = random.sample(index, train_size)\n",
    "    valid_idx = [ele for ele in index if ele not in train_idx]\n",
    " \n",
    "\n",
    "    if (number < (training_size + testing_size)):\n",
    "    \n",
    "        x_train_sub = x_train_sub + [value for pos, value in enumerate(xtrain) if pos in train_idx]\n",
    "        x_valid_sub = x_valid_sub + [value for pos, value in enumerate(xtrain) if pos in valid_idx]\n",
    "    \n",
    "        num_of_increase_training = training_size - len(train_idx)\n",
    "    \n",
    "        for i in range(0, num_of_increase_training):\n",
    "            sample_idx = random.sample(train_idx, 1)[0]\n",
    "            sample = xtrain[sample_idx]\n",
    "    \n",
    "            start_point = random.sample(range(0, int(len(sample)/2)), 1)[0]\n",
    "            end_point = random.sample(range(start_point + int(len(sample)/2), len(sample)), 1)[0]\n",
    "            new_sample = [sample[start_point: end_point]]\n",
    "            x_train_sub = x_train_sub + new_sample\n",
    "        \n",
    "        num_of_increase_valid = testing_size - len(valid_idx)\n",
    "    \n",
    "        for i in range(0, num_of_increase_valid):\n",
    "            sample_idx = random.sample(valid_idx, 1)[0]\n",
    "            sample = xtrain[sample_idx]\n",
    "    \n",
    "            start_point = random.sample(range(0, int(len(sample)/2)), 1)[0]\n",
    "            end_point = random.sample(range(start_point + int(len(sample)/2), len(sample)), 1)[0]\n",
    "            new_sample = [sample[start_point: end_point]]\n",
    "            x_valid_sub = x_valid_sub + new_sample    \n",
    "\n",
    "    else:\n",
    "        train_idx = random.sample(train_idx, training_size)\n",
    "        valid_idx = random.sample(valid_idx, testing_size)\n",
    "    \n",
    "        x_train_sub = x_train_sub + [value for pos, value in enumerate(xtrain) if pos in train_idx]\n",
    "        x_valid_sub = x_valid_sub + [value for pos, value in enumerate(xtrain) if pos in valid_idx] \n",
    "\n",
    "    \n",
    "    y_train_sub = [category for i in range(0, training_size)]\n",
    "    y_valid_sub = [category for i in range(0, testing_size)]\n",
    "    \n",
    "    return x_train_sub, x_valid_sub, y_train_sub, y_valid_sub\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_size = 4500\n",
    "testing_size = 500\n",
    "x_train = []\n",
    "x_valid = []\n",
    "y_train = []\n",
    "y_valid = []\n",
    "\n",
    "for category in range(0,12):\n",
    "    x_train_sub, x_valid_sub, y_train_sub, y_valid_sub = sampling(category, training_size, testing_size)\n",
    "    x_train = x_train + x_train_sub\n",
    "    x_valid = x_valid + x_valid_sub\n",
    "    y_train = y_train + y_train_sub\n",
    "    y_valid = y_valid + y_valid_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples in x_train:54000\n",
      "number of samples in x_valid:6000\n",
      "number of samples in y_train:54000\n",
      "number of samples in y_valid:6000\n"
     ]
    }
   ],
   "source": [
    "print(\"number of samples in x_train:\" + str(len(x_train)))\n",
    "print(\"number of samples in x_valid:\" + str(len(x_valid)))\n",
    "print(\"number of samples in y_train:\" + str(len(y_train)))\n",
    "print(\"number of samples in y_valid:\" + str(len(y_valid)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Step 1.4 Convert text into matrix ** <br>\n",
    "As the maximum length of text in the training set is 452, and the number of unique alphabets in the training set is 27, each line of text will be converted into a matrix with size (452, 27, 1), as shown in Figure 4.\n",
    "<center><img src=\"images/matrix1.png\" style=\"width:400px;height:200px;\"></center>\n",
    "<center>Figure 4 Illustration of encoding text</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def X_one_hot(data, vocab, seq_length):\n",
    "    \n",
    "    X_train = np.zeros(shape=[len(data), seq_length, len(vocab), 1])\n",
    "    \n",
    "    for i in range(0, len(data)):\n",
    "        ith_seq = data[i]\n",
    "\n",
    "        if (len(ith_seq) < seq_length):\n",
    "            ith_seq = ith_seq + ('\\n' * (seq_length - len(ith_seq)))\n",
    "        else:\n",
    "            ith_seq = ith_seq[0:(seq_length-1)]\n",
    "        \n",
    "        for j in range(0, len(ith_seq)):\n",
    "            X_train[i][j][char_to_ix[ith_seq[j]]][0] = 1\n",
    "    \n",
    "    return X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def y_one_hot(labels, num_of_classes):\n",
    "    \n",
    "    num_of_classes = tf.constant(num_of_classes, name='num_of_classes')\n",
    "    one_hot_matrix = tf.one_hot(indices=labels, depth=num_of_classes, axis=0)\n",
    "    sess = tf.Session()\n",
    "    one_hot = sess.run(one_hot_matrix)\n",
    "    sess.close()\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples = 54000\n",
      "number of test examples = 3000\n",
      "X_train shape: (54000, 452, 27, 1)\n",
      "y_train shape: (54000, 12)\n",
      "X_validation shape: (6000, 452, 27, 1)\n",
      "y_validation shape: (6000, 12)\n",
      "X_test shape: (3000, 452, 27, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_one_hot(x_train, chars, max_seq_length)\n",
    "X_valid = X_one_hot(x_valid, chars, max_seq_length)\n",
    "X_test = X_one_hot(xtest, chars, max_seq_length)\n",
    "\n",
    "y_train = y_one_hot(y_train, num_of_classes = 12).T\n",
    "y_valid = y_one_hot(y_valid, num_of_classes = 12).T\n",
    "\n",
    "\n",
    "print (\"number of training examples = \" + str(X_train.shape[0]))\n",
    "print (\"number of test examples = \" + str(X_test.shape[0]))\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"y_train shape: \" + str(y_train.shape))\n",
    "print (\"X_validation shape: \" + str(X_valid.shape))\n",
    "print (\"y_validation shape: \" + str(y_valid.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2: Create Placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_placeholders(n_H0, n_W0, n_y):\n",
    "\n",
    "    X = tf.placeholder(tf.float32, shape = [None, n_H0, n_W0, 1], name = 'input_x')\n",
    "    Y = tf.placeholder(tf.float32, [None, n_y], name = 'input_y')\n",
    "    dropout_keep_prob = tf.placeholder(tf.float32, name = 'dropout_keep_prob')\n",
    "    \n",
    "    return X, Y, dropout_keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = Tensor(\"input_x:0\", shape=(?, 452, 27, 1), dtype=float32)\n",
      "Y = Tensor(\"input_y:0\", shape=(?, 12), dtype=float32)\n",
      "Dropout_keep_prob = Tensor(\"dropout_keep_prob:0\", dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "X, Y, dropout_keep_prob = create_placeholders(max_seq_length, vocab_size,n_y = 12)\n",
    "print (\"X = \" + str(X))\n",
    "print (\"Y = \" + str(Y))\n",
    "print (\"Dropout_keep_prob = \" + str(dropout_keep_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3: Define a function for forward propagation\n",
    "Due to the time and resource constraint, the ConvNets model contains 5 layers, with 3 convolutional layers and 2 fully-connected layers. The convolutional layers have stride 1 and pooling layers are all non-overlapping. The details of each layer is shown in following tables.<br>\n",
    "<br>\n",
    "<center>Table 1: Convolutional layers description</center><br>\n",
    "<center><img src=\"images/ConvLayer.png\" style=\"width:500px;height:120px;\"></center>\n",
    "<br>\n",
    "<center>Table 2: Fully-connected layers description</center><br>\n",
    "<center><img src=\"images/FullyConnectedLayer2.png\" style=\"width:400px;height:80px;\"></center>\n",
    "\n",
    "Code in the following cell refers to: https://github.com/lc222/char-cnn-text-classification-tensorflow/blob/master/char_cnn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X, n_y, dropout_keep_prob):\n",
    "    conv_layers = [\n",
    "                    [256, 7, 3],\n",
    "                    #[256, 7, 3],\n",
    "                    #[256, 3, None],\n",
    "                    #[256, 3, None],\n",
    "                    [256, 3, None],\n",
    "                    [256, 3, 3]]\n",
    "                    \n",
    "\n",
    "    fully_layers = [1024]\n",
    "    \n",
    "    var_id = 0\n",
    "    \n",
    "    parameters = {}\n",
    "    \n",
    "    # Convolution layer\n",
    "    for i, cl in enumerate(conv_layers):\n",
    "        var_id += 1 \n",
    "        filter_width = X.get_shape()[2].value\n",
    "        filter_shape = [cl[1], filter_width, 1, cl[0]] # Perform 1D conv with [kw, inputFrameSize (i.e alphabet_size), outputFrameSize]\n",
    "        stdv = 1/np.sqrt(cl[0]*cl[1])\n",
    "        W = tf.Variable(tf.random_uniform(filter_shape, minval=-stdv, maxval=stdv), dtype='float32', name='W'+ str(var_id) ) # The kernel of the conv layer is a trainable vraiable\n",
    "        b = tf.Variable(tf.random_uniform(shape=[cl[0]], minval=-stdv, maxval=stdv), name = 'b' + str(var_id)) # and the biases as well\n",
    "        parameters['W' + str(var_id)] = W\n",
    "        parameters['b' + str(var_id)] = b\n",
    "\n",
    "        conv = tf.nn.conv2d(X, W, [1, 1, 1, 1], \"VALID\", name = \"Conv\") # Perform the convolution operation\n",
    "\n",
    "        X = tf.nn.bias_add(conv, b)\n",
    "\n",
    "        if not cl[-1] is None:\n",
    "            pool = tf.nn.max_pool(X, ksize=[1, cl[-1], 1, 1], strides=[1, cl[-1], 1, 1], padding='VALID')\n",
    "            X = tf.transpose(pool, [0, 1, 3, 2]) \n",
    "        else:\n",
    "            X = tf.transpose(X, [0, 1, 3, 2], name='tr%d' % var_id)\n",
    "\n",
    "    vec_dim = X.get_shape()[1].value * X.get_shape()[2].value\n",
    "    X = tf.reshape(X, [-1, vec_dim])\n",
    "    weights = [vec_dim] + list(fully_layers)\n",
    "    \n",
    "    # Fully-connected layer\n",
    "    for i, fl in enumerate(fully_layers):\n",
    "        var_id += 1\n",
    "        stdv = 1/np.sqrt(weights[i])\n",
    "        W = tf.Variable(tf.random_uniform([weights[i], fl], minval=-stdv, maxval=stdv), dtype='float32', name='W')\n",
    "        b = tf.Variable(tf.random_uniform(shape=[fl], minval=-stdv, maxval=stdv), dtype='float32', name = 'b')\n",
    "        parameters['W' + str(var_id)] = W\n",
    "        parameters['b' + str(var_id)] = b\n",
    "                \n",
    "        X = tf.nn.xw_plus_b(X, W, b)\n",
    "        X = tf.nn.dropout(X, dropout_keep_prob)\n",
    "    \n",
    "    stdv = 1/np.sqrt(weights[-1])\n",
    "    W = tf.Variable(tf.random_uniform([weights[-1], n_y], minval=-stdv, maxval=stdv), dtype='float32', name='W')\n",
    "    b = tf.Variable(tf.random_uniform(shape=[n_y], minval=-stdv, maxval=stdv), name = 'b')\n",
    "    parameters['W' + str(var_id + 1)] = W\n",
    "    parameters['b' + str(var_id + 1)] = b\n",
    "    p_y_given_x = tf.nn.xw_plus_b(X, W, b, name=\"scores\")\n",
    "        \n",
    "    return p_y_given_x, parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_y_given_x = [[ 0.05735406  0.02709517  0.01198866  0.03233381  0.00304832 -0.03089858\n",
      "  -0.00684476  0.00375878 -0.01954173  0.03440209 -0.01253452  0.05587472]\n",
      " [-0.00113846  0.01628569 -0.0179968  -0.00666194  0.0297815  -0.01148325\n",
      "   0.01343633  0.00485595 -0.03527058  0.00794092 -0.00568362  0.06436133]]\n"
     ]
    }
   ],
   "source": [
    "#tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    X, Y, dropout_keep_prob = create_placeholders(max_seq_length, vocab_size,n_y = 12)\n",
    "    p_y_given_x = forward_propagation(X, 12, dropout_keep_prob)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    res1, res2 = sess.run(p_y_given_x, {X: np.random.randn(2,452,27,1), dropout_keep_prob:0.5})\n",
    "    print(\"p_y_given_x = \" + str(res1))\n",
    "    #print(res2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4: Define a function to compute cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(X, Y):\n",
    "    \n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=X, labels=Y))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-18-2a37f36569ce>:3: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "cost = 140.47882\n"
     ]
    }
   ],
   "source": [
    "#tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    X, Y, dropout_keep_prob = create_placeholders(max_seq_length, vocab_size,n_y = 12)\n",
    "    p_y_given_x, parameters = forward_propagation(X, 12, dropout_keep_prob)\n",
    "    cost = compute_cost(p_y_given_x, Y)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    res2 = sess.run(cost, {X: np.random.randn(2,452,27,1), dropout_keep_prob:0.5, Y: np.random.randint(0, 11, size = (2,12))})\n",
    "    print(\"cost = \" + str(res2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step5: Define a function to divide mini batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size):\n",
    "\n",
    "    \n",
    "    m = X.shape[0]                  # number of training examples\n",
    "    mini_batches = []\n",
    "        \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[permutation, :]\n",
    "    shuffled_Y = Y[permutation, :]\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[k * mini_batch_size:(k + 1) * mini_batch_size, :]\n",
    "        mini_batch_Y = shuffled_Y[k * mini_batch_size:(k + 1) * mini_batch_size, :]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        end = m - mini_batch_size * math.floor(m / mini_batch_size)\n",
    "        mini_batch_X = shuffled_X[num_complete_minibatches * mini_batch_size:, :]\n",
    "        mini_batch_Y = shuffled_Y[num_complete_minibatches * mini_batch_size:, :]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the 1st mini_batch_X: (64, 452, 27, 1)\n",
      "shape of the 2nd mini_batch_X: (64, 452, 27, 1)\n",
      "shape of the 3rd mini_batch_X: (64, 452, 27, 1)\n",
      "shape of the 1st mini_batch_Y: (64, 12)\n",
      "shape of the 2nd mini_batch_Y: (64, 12)\n",
      "shape of the 3rd mini_batch_Y: (64, 12)\n"
     ]
    }
   ],
   "source": [
    "mini_batches = random_mini_batches(X_train, y_train, 64)\n",
    "\n",
    "print(\"shape of the 1st mini_batch_X: \" + str(mini_batches[0][0].shape))\n",
    "print(\"shape of the 2nd mini_batch_X: \" + str(mini_batches[1][0].shape))\n",
    "print(\"shape of the 3rd mini_batch_X: \" + str(mini_batches[2][0].shape))\n",
    "print(\"shape of the 1st mini_batch_Y: \" + str(mini_batches[0][1].shape))\n",
    "print(\"shape of the 2nd mini_batch_Y: \" + str(mini_batches[1][1].shape)) \n",
    "print(\"shape of the 3rd mini_batch_Y: \" + str(mini_batches[2][1].shape))\n",
    "#print(\"mini batch sanity check: \" + str(mini_batches[0][0][0][0:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step6ï¼šModelling\n",
    "Parameters of this model include:\n",
    "- learning rate = 0.0001\n",
    "- number of epochs = 120\n",
    "- minibatch_size = 1024\n",
    "- drop out probibility = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, learning_rate=0.0001,\n",
    "          num_epochs=120, minibatch_size=1024, print_cost=True):\n",
    "    \n",
    "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    (m, n_H0, n_W0, n_C0) = X_train.shape             \n",
    "    n_y = Y_train.shape[1]                            \n",
    "    costs = []    \n",
    "    conv_layers = [\n",
    "                    [256, 7, 3],\n",
    "                    #[256, 7, 3],\n",
    "                    #[256, 3, None],\n",
    "                    #[256, 3, None],\n",
    "                    [256, 3, None],\n",
    "                    [256, 3, 3]\n",
    "                    ]\n",
    "\n",
    "    fully_layers = [1024]\n",
    "    # Create Placeholders of the correct shape\n",
    "    X, Y, dropout_keep_prob = create_placeholders(n_H0, n_W0, n_y)\n",
    "\n",
    "    \n",
    "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
    "    p_y_given_x, parameters = forward_propagation(X, n_y, dropout_keep_prob)\n",
    "    \n",
    "    # Cost function: Add cost function to tensorflow graph\n",
    "    cost = compute_cost(p_y_given_x, Y)\n",
    "    \n",
    "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer that minimizes the cost.\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    \n",
    "    # Initialize all the variables globally\n",
    "    init = tf.global_variables_initializer()\n",
    "     \n",
    "    # Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Run the initialization\n",
    "        sess.run(init)\n",
    "        \n",
    "        # Do the training loop\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            minibatch_cost = 0.\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "\n",
    "                # Select a minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                # IMPORTANT: The line that runs the graph on a minibatch.\n",
    "                # Run the session to execute the optimizer and the cost, the feedict should contain a minibatch for (X,Y).\n",
    "    \n",
    "                _ , temp_cost = sess.run([optimizer, cost], feed_dict={X:minibatch_X, Y:minibatch_Y,dropout_keep_prob:[0.5]})\n",
    "                \n",
    "                minibatch_cost += temp_cost / num_minibatches\n",
    "                \n",
    "\n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, minibatch_cost))\n",
    "            if print_cost == True and epoch % 1 == 0:\n",
    "                costs.append(minibatch_cost)\n",
    "            if minibatch_cost < 0.01:\n",
    "                break\n",
    "                \n",
    "        \n",
    "        \n",
    "        # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "        # Save the parameters in a variable\n",
    "        parameters = sess.run(parameters)\n",
    "        print(\"Parameters have been trained!\")\n",
    "\n",
    "        # Calculate the correct predictions\n",
    "        predict_op = tf.argmax(p_y_given_x, 1)\n",
    "        correct_prediction = tf.equal(predict_op, tf.argmax(Y, 1))\n",
    "        \n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        train_accuracy = accuracy.eval({X: X_train, Y: Y_train, dropout_keep_prob: 0.5})\n",
    "        test_accuracy = accuracy.eval({X: X_test, Y: Y_test, dropout_keep_prob: 0.5})\n",
    "        print(\"Train Accuracy:\", train_accuracy)\n",
    "        print(\"Test Accuracy:\", test_accuracy)\n",
    "                \n",
    "        return train_accuracy, test_accuracy, parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 2.394825\n",
      "Cost after epoch 5: 1.968447\n",
      "Cost after epoch 10: 1.592282\n",
      "Cost after epoch 15: 1.296215\n",
      "Cost after epoch 20: 1.142729\n",
      "Cost after epoch 25: 1.026083\n",
      "Cost after epoch 30: 0.941631\n",
      "Cost after epoch 35: 0.870623\n",
      "Cost after epoch 40: 0.789543\n",
      "Cost after epoch 45: 0.719298\n",
      "Cost after epoch 50: 0.661522\n",
      "Cost after epoch 55: 0.614659\n",
      "Cost after epoch 60: 0.555133\n",
      "Cost after epoch 65: 0.498714\n",
      "Cost after epoch 70: 0.451806\n",
      "Cost after epoch 75: 0.406383\n",
      "Cost after epoch 80: 0.347689\n",
      "Cost after epoch 85: 0.317224\n",
      "Cost after epoch 90: 0.275441\n",
      "Cost after epoch 95: 0.229052\n",
      "Cost after epoch 100: 0.185574\n",
      "Cost after epoch 105: 0.156622\n",
      "Cost after epoch 110: 0.121651\n",
      "Cost after epoch 115: 0.097044\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x264163abef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.98205554\n",
      "Test Accuracy: 0.64966667\n",
      "Time used: 24716.676879426468\n"
     ]
    }
   ],
   "source": [
    "start = time.clock()\n",
    "_, _, parameters = model(X_train, y_train, X_valid, y_valid)\n",
    "elapsed = (time.clock() - start)\n",
    "print(\"Time used:\",elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step7: Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation_for_predict(X, parameters, dropout_keep_prob):\n",
    "\n",
    "    conv_layers = [\n",
    "                    [256, 7, 3],\n",
    "                    #[256, 7, 3],\n",
    "                    #[256, 3, None],\n",
    "                    #[256, 3, None],\n",
    "                    [256, 3, None],\n",
    "                    [256, 3, 3]]\n",
    "                    \n",
    "\n",
    "    fully_layers = [1024]\n",
    "    \n",
    "    var_id = 0\n",
    "        \n",
    "    for i, cl in enumerate(conv_layers):\n",
    "        var_id += 1\n",
    "\n",
    "        W = params[\"W\" + str(var_id)]\n",
    "        b = params[\"b\" + str(var_id)]\n",
    "\n",
    "        conv = tf.nn.conv2d(X, W, [1, 1, 1, 1], \"VALID\", name = \"Conv\") # Perform the convolution operation\n",
    "\n",
    "        X = tf.nn.bias_add(conv, b)\n",
    "\n",
    "        if not cl[-1] is None:\n",
    "            pool = tf.nn.max_pool(X, ksize=[1, cl[-1], 1, 1], strides=[1, cl[-1], 1, 1], padding='VALID')\n",
    "            X = tf.transpose(pool, [0, 1, 3, 2]) \n",
    "        else:\n",
    "            X = tf.transpose(X, [0, 1, 3, 2], name='tr%d' % var_id)\n",
    "\n",
    "    vec_dim = X.get_shape()[1].value * X.get_shape()[2].value\n",
    "    X = tf.reshape(X, [-1, vec_dim])\n",
    "    weights = [vec_dim] + list(fully_layers)\n",
    "    \n",
    "    for i, fl in enumerate(fully_layers):\n",
    "        var_id += 1\n",
    "\n",
    "        W = params[\"W\" + str(var_id)]\n",
    "        b = params[\"b\" + str(var_id)]\n",
    "                \n",
    "        X = tf.nn.xw_plus_b(X, W, b)\n",
    "        X = tf.nn.dropout(X, dropout_keep_prob)\n",
    "    \n",
    "    W = params[\"W\" + str(var_id + 1)]\n",
    "    b = params[\"b\" + str(var_id + 1)]\n",
    "\n",
    "\n",
    "    p_y_given_x = tf.nn.xw_plus_b(X, W, b, name=\"scores\")  \n",
    "    \n",
    "    return p_y_given_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(X, parameters):\n",
    "    \n",
    "    params = {}\n",
    "    \n",
    "    for key in parameters.keys():\n",
    "        params[key] = tf.convert_to_tensor(parameters[key])\n",
    "\n",
    "        \n",
    "    x = tf.placeholder(tf.float32, shape = [None, 452, 27, 1], name = 'input_x')\n",
    "    dropout_keep_prob = tf.placeholder(tf.float32, name = 'dropout_keep_prob')      \n",
    "\n",
    "    predictions = forward_propagation_for_predict(x, params, dropout_keep_prob)\n",
    "\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        predictions = sess.run(predictions, feed_dict = {x: X, dropout_keep_prob:0.5})\n",
    "        predicted_value = tf.argmax(predictions,1)\n",
    "    \n",
    "    \n",
    "    return predicted_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_value = predict(X_test, parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables()) #execute init_op\n",
    "    #print the random values that we sample\n",
    "    res = sess.run(predicted_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt('ytest_v10.txt', res, fmt='%.1d')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
