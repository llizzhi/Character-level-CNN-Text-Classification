{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.python.framework import ops\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Files and Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = open('xtrain_obfuscated.txt', 'r').read().split('\\n')[:-1]\n",
    "xtest = open('xtest_obfuscated.txt', 'r').read().split('\\n')[:-1]\n",
    "ytrain = open('ytrain.txt', 'r').read().split('\\n')[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 27 unique characters in your data.\n"
     ]
    }
   ],
   "source": [
    "data = open('xtrain_obfuscated.txt', 'r').read()\n",
    "chars = list(set(data))\n",
    "vocab_size = len(chars)\n",
    "print('There are %d unique characters in your data.' % (vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n"
     ]
    }
   ],
   "source": [
    "char_to_ix = { ch:i for i,ch in enumerate(sorted(chars)) }\n",
    "print(char_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum length of string in your dataset is 452\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = max([len(i) for i in xtrain])\n",
    "print('The maximum length of string in your dataset is %d' % (max_seq_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of value 0:543\n",
      "number of value 1:3459\n",
      "number of value 2:1471\n",
      "number of value 3:4023\n",
      "number of value 4:2337\n",
      "number of value 5:2283\n",
      "number of value 6:4226\n",
      "number of value 7:5097\n",
      "number of value 8:3634\n",
      "number of value 9:980\n",
      "number of value 10:3052\n",
      "number of value 11:1408\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,12):\n",
    "    print('number of value ' + str(i) + ':' + str(ytrain.count(str(i))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(category, training_size, testing_size):\n",
    "    x_train_sub = []\n",
    "    y_train_sub = []\n",
    "    x_valid_sub = []\n",
    "    y_valid_sub = []\n",
    "\n",
    "    index = [pos for pos, value in enumerate(ytrain) if value == str(category)]\n",
    "    number = len(index)\n",
    "    train_size = round(number * 0.9)\n",
    "\n",
    "    train_idx = random.sample(index, train_size)\n",
    "    valid_idx = [ele for ele in index if ele not in train_idx]\n",
    " \n",
    "\n",
    "    if (number < (training_size + testing_size)):\n",
    "    \n",
    "        x_train_sub = x_train_sub + [value for pos, value in enumerate(xtrain) if pos in train_idx]\n",
    "        x_valid_sub = x_valid_sub + [value for pos, value in enumerate(xtrain) if pos in valid_idx]\n",
    "    \n",
    "        num_of_increase_training = training_size - len(train_idx)\n",
    "    \n",
    "        for i in range(0, num_of_increase_training):\n",
    "            sample_idx = random.sample(train_idx, 1)[0]\n",
    "            sample = xtrain[sample_idx]\n",
    "    \n",
    "            start_point = random.sample(range(0, int(len(sample)/2)), 1)[0]\n",
    "            end_point = random.sample(range(start_point + int(len(sample)/2), len(sample)), 1)[0]\n",
    "            new_sample = [sample[start_point: end_point]]\n",
    "            x_train_sub = x_train_sub + new_sample\n",
    "        \n",
    "        num_of_increase_valid = testing_size - len(valid_idx)\n",
    "    \n",
    "        for i in range(0, num_of_increase_valid):\n",
    "            sample_idx = random.sample(valid_idx, 1)[0]\n",
    "            sample = xtrain[sample_idx]\n",
    "    \n",
    "            start_point = random.sample(range(0, int(len(sample)/2)), 1)[0]\n",
    "            end_point = random.sample(range(start_point + int(len(sample)/2), len(sample)), 1)[0]\n",
    "            new_sample = [sample[start_point: end_point]]\n",
    "            x_valid_sub = x_valid_sub + new_sample    \n",
    "\n",
    "    else:\n",
    "        train_idx = random.sample(train_idx, training_size)\n",
    "        valid_idx = random.sample(valid_idx, testing_size)\n",
    "    \n",
    "        x_train_sub = x_train_sub + [value for pos, value in enumerate(xtrain) if pos in train_idx]\n",
    "        x_valid_sub = x_valid_sub + [value for pos, value in enumerate(xtrain) if pos in valid_idx] \n",
    "\n",
    "    \n",
    "    y_train_sub = [category for i in range(0, training_size)]\n",
    "    y_valid_sub = [category for i in range(0, testing_size)]\n",
    "    \n",
    "    return x_train_sub, x_valid_sub, y_train_sub, y_valid_sub\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_size = 4500\n",
    "testing_size = 500\n",
    "x_train = []\n",
    "x_valid = []\n",
    "y_train = []\n",
    "y_valid = []\n",
    "\n",
    "for category in range(0,12):\n",
    "    x_train_sub, x_valid_sub, y_train_sub, y_valid_sub = sampling(category, training_size, testing_size)\n",
    "    x_train = x_train + x_train_sub\n",
    "    x_valid = x_valid + x_valid_sub\n",
    "    y_train = y_train + y_train_sub\n",
    "    y_valid = y_valid + y_valid_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples in x_train:54000\n",
      "number of samples in x_valid:6000\n",
      "number of samples in y_train:54000\n",
      "number of samples in y_valid:6000\n"
     ]
    }
   ],
   "source": [
    "print(\"number of samples in x_train:\" + str(len(x_train)))\n",
    "print(\"number of samples in x_valid:\" + str(len(x_valid)))\n",
    "print(\"number of samples in y_train:\" + str(len(y_train)))\n",
    "print(\"number of samples in y_valid:\" + str(len(y_valid)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def X_one_hot(data, vocab, seq_length):\n",
    "    \n",
    "    X_train = np.zeros(shape=[len(data), seq_length, len(vocab), 1])\n",
    "    \n",
    "    for i in range(0, len(data)):\n",
    "        ith_seq = data[i]\n",
    "\n",
    "        if (len(ith_seq) < seq_length):\n",
    "            ith_seq = ith_seq + ('\\n' * (seq_length - len(ith_seq)))\n",
    "        else:\n",
    "            ith_seq = ith_seq[0:(seq_length-1)]\n",
    "        \n",
    "        for j in range(0, len(ith_seq)):\n",
    "            X_train[i][j][char_to_ix[ith_seq[j]]][0] = 1\n",
    "    \n",
    "    return X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_one_hot(labels, num_of_classes):\n",
    "    \n",
    "    num_of_classes = tf.constant(num_of_classes, name='num_of_classes')\n",
    "    one_hot_matrix = tf.one_hot(indices=labels, depth=num_of_classes, axis=0)\n",
    "    sess = tf.Session()\n",
    "    one_hot = sess.run(one_hot_matrix)\n",
    "    sess.close()\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples = 54000\n",
      "number of test examples = 3000\n",
      "X_train shape: (54000, 452, 27, 1)\n",
      "y_train shape: (54000, 12)\n",
      "X_validation shape: (6000, 452, 27, 1)\n",
      "y_validation shape: (6000, 12)\n",
      "X_test shape: (3000, 452, 27, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_one_hot(x_train, chars, max_seq_length)\n",
    "X_valid = X_one_hot(x_valid, chars, max_seq_length)\n",
    "X_test = X_one_hot(xtest, chars, max_seq_length)\n",
    "\n",
    "y_train = y_one_hot(y_train, num_of_classes = 12).T\n",
    "y_valid = y_one_hot(y_valid, num_of_classes = 12).T\n",
    "\n",
    "\n",
    "print (\"number of training examples = \" + str(X_train.shape[0]))\n",
    "print (\"number of test examples = \" + str(X_test.shape[0]))\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"y_train shape: \" + str(y_train.shape))\n",
    "print (\"X_validation shape: \" + str(X_valid.shape))\n",
    "print (\"y_validation shape: \" + str(y_valid.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholders(n_H0, n_W0, n_y):\n",
    "\n",
    "    X = tf.placeholder(tf.float32, shape = [None, n_H0, n_W0, 1], name = 'input_x')\n",
    "    Y = tf.placeholder(tf.float32, [None, n_y], name = 'input_y')\n",
    "    dropout_keep_prob = tf.placeholder(tf.float32, name = 'dropout_keep_prob')\n",
    "    \n",
    "    return X, Y, dropout_keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = Tensor(\"input_x:0\", shape=(?, 452, 27, 1), dtype=float32)\n",
      "Y = Tensor(\"input_y:0\", shape=(?, 12), dtype=float32)\n",
      "Dropout_keep_prob = Tensor(\"dropout_keep_prob:0\", dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "X, Y, dropout_keep_prob = create_placeholders(max_seq_length, vocab_size,n_y = 12)\n",
    "print (\"X = \" + str(X))\n",
    "print (\"Y = \" + str(Y))\n",
    "print (\"Dropout_keep_prob = \" + str(dropout_keep_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, n_y, dropout_keep_prob):\n",
    "    conv_layers = [\n",
    "                    [256, 7, 3],\n",
    "                    #[256, 7, 3],\n",
    "                    #[256, 3, None],\n",
    "                    #[256, 3, None],\n",
    "                    [256, 3, None],\n",
    "                    [256, 3, 3]]\n",
    "                    \n",
    "\n",
    "    #fully_layers = [1024, 1024]\n",
    "    fully_layers = [1024]\n",
    "    \n",
    "    var_id = 0\n",
    "    \n",
    "    parameters = {}\n",
    "    \n",
    "    for i, cl in enumerate(conv_layers):\n",
    "        var_id += 1 \n",
    "        filter_width = X.get_shape()[2].value\n",
    "        filter_shape = [cl[1], filter_width, 1, cl[0]] # Perform 1D conv with [kw, inputFrameSize (i.e alphabet_size), outputFrameSize]\n",
    "                # Convolution layer\n",
    "        stdv = 1/np.sqrt(cl[0]*cl[1])\n",
    "        W = tf.Variable(tf.random_uniform(filter_shape, minval=-stdv, maxval=stdv), dtype='float32', name='W'+ str(var_id) ) # The kernel of the conv layer is a trainable vraiable\n",
    "        b = tf.Variable(tf.random_uniform(shape=[cl[0]], minval=-stdv, maxval=stdv), name = 'b' + str(var_id)) # and the biases as well\n",
    "        parameters['W' + str(var_id)] = W\n",
    "        parameters['b' + str(var_id)] = b\n",
    "\n",
    "        conv = tf.nn.conv2d(X, W, [1, 1, 1, 1], \"VALID\", name = \"Conv\") # Perform the convolution operation\n",
    "\n",
    "        X = tf.nn.bias_add(conv, b)\n",
    "\n",
    "        if not cl[-1] is None:\n",
    "            pool = tf.nn.max_pool(X, ksize=[1, cl[-1], 1, 1], strides=[1, cl[-1], 1, 1], padding='VALID')\n",
    "            X = tf.transpose(pool, [0, 1, 3, 2]) # [batch_size, img_width, img_height, 1]\n",
    "        else:\n",
    "            X = tf.transpose(X, [0, 1, 3, 2], name='tr%d' % var_id) # [batch_size, img_width, img_height, 1]\n",
    "\n",
    "    vec_dim = X.get_shape()[1].value * X.get_shape()[2].value\n",
    "    X = tf.reshape(X, [-1, vec_dim])\n",
    "    weights = [vec_dim] + list(fully_layers)\n",
    "    \n",
    "    for i, fl in enumerate(fully_layers):\n",
    "        var_id += 1\n",
    "        stdv = 1/np.sqrt(weights[i])\n",
    "        W = tf.Variable(tf.random_uniform([weights[i], fl], minval=-stdv, maxval=stdv), dtype='float32', name='W')\n",
    "        b = tf.Variable(tf.random_uniform(shape=[fl], minval=-stdv, maxval=stdv), dtype='float32', name = 'b')\n",
    "        parameters['W' + str(var_id)] = W\n",
    "        parameters['b' + str(var_id)] = b\n",
    "                \n",
    "        X = tf.nn.xw_plus_b(X, W, b)\n",
    "        X = tf.nn.dropout(X, dropout_keep_prob)\n",
    "    \n",
    "    stdv = 1/np.sqrt(weights[-1])\n",
    "    W = tf.Variable(tf.random_uniform([weights[-1], n_y], minval=-stdv, maxval=stdv), dtype='float32', name='W')\n",
    "    b = tf.Variable(tf.random_uniform(shape=[n_y], minval=-stdv, maxval=stdv), name = 'b')\n",
    "    parameters['W' + str(var_id + 1)] = W\n",
    "    parameters['b' + str(var_id + 1)] = b\n",
    "    p_y_given_x = tf.nn.xw_plus_b(X, W, b, name=\"scores\")\n",
    "        \n",
    "    return p_y_given_x, parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_y_given_x = [[ 0.0241223   0.02751884 -0.01611185  0.0312215  -0.09325986 -0.0704421\n",
      "  -0.00981938  0.03025233  0.01550109 -0.00709374 -0.03389454  0.0571539 ]\n",
      " [-0.03218047 -0.00753319  0.06038624  0.01163254 -0.04228123 -0.07713233\n",
      "  -0.08065082  0.01224003  0.00050247 -0.03238136 -0.00233207  0.00563744]]\n"
     ]
    }
   ],
   "source": [
    "#tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    X, Y, dropout_keep_prob = create_placeholders(max_seq_length, vocab_size,n_y = 12)\n",
    "    p_y_given_x = forward_propagation(X, 12, dropout_keep_prob)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    res1, res2 = sess.run(p_y_given_x, {X: np.random.randn(2,452,27,1), dropout_keep_prob:0.5})\n",
    "    print(\"p_y_given_x = \" + str(res1))\n",
    "    #print(res2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, Y):\n",
    "    \n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=X, labels=Y))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-17-2a37f36569ce>:3: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "cost = 135.08829\n"
     ]
    }
   ],
   "source": [
    "#tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    X, Y, dropout_keep_prob = create_placeholders(max_seq_length, vocab_size,n_y = 12)\n",
    "    p_y_given_x, parameters = forward_propagation(X, 12, dropout_keep_prob)\n",
    "    cost = compute_cost(p_y_given_x, Y)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    res2 = sess.run(cost, {X: np.random.randn(2,452,27,1), dropout_keep_prob:0.5, Y: np.random.randint(0, 11, size = (2,12))})\n",
    "    print(\"cost = \" + str(res2))\n",
    "    #print(\"p_y_given_x = \" + str(res1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size):\n",
    "\n",
    "    \n",
    "    m = X.shape[0]                  # number of training examples\n",
    "    mini_batches = []\n",
    "        \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[permutation, :]\n",
    "    shuffled_Y = Y[permutation, :]\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        ### START CODE HERE ### (approx. 2 lines)\n",
    "        mini_batch_X = shuffled_X[k * mini_batch_size:(k + 1) * mini_batch_size, :]\n",
    "        mini_batch_Y = shuffled_Y[k * mini_batch_size:(k + 1) * mini_batch_size, :]\n",
    "        ### END CODE HERE ###\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        ### START CODE HERE ### (approx. 2 lines)\n",
    "        end = m - mini_batch_size * math.floor(m / mini_batch_size)\n",
    "        mini_batch_X = shuffled_X[num_complete_minibatches * mini_batch_size:, :]\n",
    "        mini_batch_Y = shuffled_Y[num_complete_minibatches * mini_batch_size:, :]\n",
    "        ### END CODE HERE ###\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the 1st mini_batch_X: (64, 452, 27, 1)\n",
      "shape of the 2nd mini_batch_X: (64, 452, 27, 1)\n",
      "shape of the 3rd mini_batch_X: (64, 452, 27, 1)\n",
      "shape of the 1st mini_batch_Y: (64, 12)\n",
      "shape of the 2nd mini_batch_Y: (64, 12)\n",
      "shape of the 3rd mini_batch_Y: (64, 12)\n"
     ]
    }
   ],
   "source": [
    "mini_batches = random_mini_batches(X_train, y_train, 64)\n",
    "\n",
    "print(\"shape of the 1st mini_batch_X: \" + str(mini_batches[0][0].shape))\n",
    "print(\"shape of the 2nd mini_batch_X: \" + str(mini_batches[1][0].shape))\n",
    "print(\"shape of the 3rd mini_batch_X: \" + str(mini_batches[2][0].shape))\n",
    "print(\"shape of the 1st mini_batch_Y: \" + str(mini_batches[0][1].shape))\n",
    "print(\"shape of the 2nd mini_batch_Y: \" + str(mini_batches[1][1].shape)) \n",
    "print(\"shape of the 3rd mini_batch_Y: \" + str(mini_batches[2][1].shape))\n",
    "#print(\"mini batch sanity check: \" + str(mini_batches[0][0][0][0:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, learning_rate=0.0001,\n",
    "          num_epochs=120, minibatch_size=1024, print_cost=True):\n",
    "    \n",
    "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    (m, n_H0, n_W0, n_C0) = X_train.shape             \n",
    "    n_y = Y_train.shape[1]                            \n",
    "    costs = []    # To keep track of the cost\n",
    "    conv_layers = [\n",
    "                    [256, 7, 3],\n",
    "                    #[256, 7, 3],\n",
    "                    #[256, 3, None],\n",
    "                    #[256, 3, None],\n",
    "                    [256, 3, None],\n",
    "                    [256, 3, 3]\n",
    "                    ]\n",
    "\n",
    "    #fully_layers = [1024, 1024]\n",
    "    fully_layers = [1024]\n",
    "    # Create Placeholders of the correct shape\n",
    "    X, Y, dropout_keep_prob = create_placeholders(n_H0, n_W0, n_y)\n",
    "\n",
    "    \n",
    "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
    "    p_y_given_x, parameters = forward_propagation(X, n_y, dropout_keep_prob)\n",
    "    \n",
    "    # Cost function: Add cost function to tensorflow graph\n",
    "    cost = compute_cost(p_y_given_x, Y)\n",
    "    \n",
    "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer that minimizes the cost.\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    \n",
    "    # Initialize all the variables globally\n",
    "    init = tf.global_variables_initializer()\n",
    "     \n",
    "    # Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Run the initialization\n",
    "        sess.run(init)\n",
    "        \n",
    "        # Do the training loop\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            minibatch_cost = 0.\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "\n",
    "                # Select a minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                # IMPORTANT: The line that runs the graph on a minibatch.\n",
    "                # Run the session to execute the optimizer and the cost, the feedict should contain a minibatch for (X,Y).\n",
    "                ### START CODE HERE ### (1 line)\n",
    "                _ , temp_cost = sess.run([optimizer, cost], feed_dict={X:minibatch_X, Y:minibatch_Y,dropout_keep_prob:[0.5]})\n",
    "                ### END CODE HERE ###\n",
    "                \n",
    "                minibatch_cost += temp_cost / num_minibatches\n",
    "                \n",
    "\n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, minibatch_cost))\n",
    "            if print_cost == True and epoch % 1 == 0:\n",
    "                costs.append(minibatch_cost)\n",
    "            if minibatch_cost < 0.01:\n",
    "                break\n",
    "                \n",
    "        \n",
    "        \n",
    "        # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "        \n",
    "        # Save the parameters in a variable\n",
    "        parameters = sess.run(parameters)\n",
    "        print(\"Parameters have been trained!\")\n",
    "\n",
    "        # Calculate the correct predictions\n",
    "        predict_op = tf.argmax(p_y_given_x, 1)\n",
    "        correct_prediction = tf.equal(predict_op, tf.argmax(Y, 1))\n",
    "        \n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        train_accuracy = accuracy.eval({X: X_train, Y: Y_train, dropout_keep_prob: 0.5})\n",
    "        test_accuracy = accuracy.eval({X: X_test, Y: Y_test, dropout_keep_prob: 0.5})\n",
    "        print(\"Train Accuracy:\", train_accuracy)\n",
    "        print(\"Test Accuracy:\", test_accuracy)\n",
    "                \n",
    "        return train_accuracy, test_accuracy, parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 2.395649\n",
      "Cost after epoch 5: 1.934077\n",
      "Cost after epoch 10: 1.564151\n",
      "Cost after epoch 15: 1.311741\n",
      "Cost after epoch 20: 1.148098\n",
      "Cost after epoch 25: 1.034505\n",
      "Cost after epoch 30: 0.963972\n",
      "Cost after epoch 35: 0.874187\n",
      "Cost after epoch 40: 0.814019\n",
      "Cost after epoch 45: 0.728223\n",
      "Cost after epoch 50: 0.665857\n",
      "Cost after epoch 55: 0.598085\n",
      "Cost after epoch 60: 0.560371\n",
      "Cost after epoch 65: 0.499431\n",
      "Cost after epoch 70: 0.440366\n",
      "Cost after epoch 75: 0.393922\n",
      "Cost after epoch 80: 0.334368\n",
      "Cost after epoch 85: 0.305104\n",
      "Cost after epoch 90: 0.251217\n",
      "Cost after epoch 95: 0.238834\n",
      "Cost after epoch 100: 0.183443\n",
      "Cost after epoch 105: 0.138311\n",
      "Cost after epoch 110: 0.132377\n",
      "Cost after epoch 115: 0.089772\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12ff4eded68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters have been trained!\n",
      "Train Accuracy: 0.99288887\n",
      "Test Accuracy: 0.668\n",
      "Time used: 23876.034982833804\n"
     ]
    }
   ],
   "source": [
    "start = time.clock()\n",
    "_, _, parameters = model(X_train, y_train, X_valid, y_valid)\n",
    "elapsed = (time.clock() - start)\n",
    "print(\"Time used:\",elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_for_predict(X, parameters, dropout_keep_prob):\n",
    "\n",
    "    conv_layers = [\n",
    "                    [256, 7, 3],\n",
    "                    #[256, 7, 3],\n",
    "                    #[256, 3, None],\n",
    "                    #[256, 3, None],\n",
    "                    [256, 3, None],\n",
    "                    [256, 3, 3]]\n",
    "                    \n",
    "\n",
    "    #fully_layers = [1024, 1024]\n",
    "    fully_layers = [1024]\n",
    "\n",
    "    #params = {}\n",
    "    \n",
    "    #for key in parameters.keys():\n",
    "        #params[key] = tf.convert_to_tensor(parameters[key])\n",
    "    \n",
    "    var_id = 0\n",
    "        \n",
    "    for i, cl in enumerate(conv_layers):\n",
    "        var_id += 1\n",
    "\n",
    "        W = parameters[\"W\" + str(var_id)]\n",
    "        b = parameters[\"b\" + str(var_id)]\n",
    "\n",
    "        conv = tf.nn.conv2d(X, W, [1, 1, 1, 1], \"VALID\", name = \"Conv\") # Perform the convolution operation\n",
    "\n",
    "        X = tf.nn.bias_add(conv, b)\n",
    "\n",
    "        if not cl[-1] is None:\n",
    "            pool = tf.nn.max_pool(X, ksize=[1, cl[-1], 1, 1], strides=[1, cl[-1], 1, 1], padding='VALID')\n",
    "            X = tf.transpose(pool, [0, 1, 3, 2]) # [batch_size, img_width, img_height, 1]\n",
    "        else:\n",
    "            X = tf.transpose(X, [0, 1, 3, 2], name='tr%d' % var_id) # [batch_size, img_width, img_height, 1]\n",
    "\n",
    "    vec_dim = X.get_shape()[1].value * X.get_shape()[2].value\n",
    "    X = tf.reshape(X, [-1, vec_dim])\n",
    "    weights = [vec_dim] + list(fully_layers)\n",
    "    \n",
    "    for i, fl in enumerate(fully_layers):\n",
    "        var_id += 1\n",
    "\n",
    "        W = parameters[\"W\" + str(var_id)]\n",
    "        b = parameters[\"b\" + str(var_id)]\n",
    "                \n",
    "        X = tf.nn.xw_plus_b(X, W, b)\n",
    "        X = tf.nn.dropout(X, dropout_keep_prob)\n",
    "    \n",
    "    W = parameters[\"W\" + str(var_id + 1)]\n",
    "    b = parameters[\"b\" + str(var_id + 1)]\n",
    "\n",
    "\n",
    "    p_y_given_x = tf.nn.xw_plus_b(X, W, b, name=\"scores\")  \n",
    "    \n",
    "    return p_y_given_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, parameters):\n",
    "    \n",
    "    params = {}\n",
    "    \n",
    "    for key in parameters.keys():\n",
    "        params[key] = tf.convert_to_tensor(parameters[key])\n",
    "\n",
    "        \n",
    "    x = tf.placeholder(tf.float32, shape = [None, 452, 27, 1], name = 'input_x')\n",
    "    dropout_keep_prob = tf.placeholder(tf.float32, name = 'dropout_keep_prob')      \n",
    "\n",
    "    predictions = forward_propagation_for_predict(x, params, dropout_keep_prob)\n",
    "\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        predictions = sess.run(predictions, feed_dict = {x: X, dropout_keep_prob:0.5})\n",
    "        predicted_value = tf.argmax(predictions,1)\n",
    "    \n",
    "    \n",
    "    return predicted_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_value = predict(X_test, parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables()) #execute init_op\n",
    "    #print the random values that we sample\n",
    "    res = sess.run(predicted_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('ytest_v10.txt', res, fmt='%.1d')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
